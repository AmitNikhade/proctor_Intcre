# import argparse
# import io
# import speech_recognition as sr
# import torch
# import asyncio
# import json
# import cv2
# from aiortc import RTCPeerConnection, RTCSessionDescription, VideoStreamTrack
# from aiortc.contrib.media import MediaRecorder
# import websockets
# from av import VideoFrame
# import logging
# from datetime import datetime
# from queue import Queue
# from tempfile import NamedTemporaryFile
# from sys import platform
# from PyQt5.QtWidgets import QApplication, QMainWindow, QVBoxLayout, QWidget, QPushButton, QHBoxLayout, QLabel, QTextEdit
# from PyQt5.QtCore import Qt, QTimer, pyqtSlot, QThread, pyqtSignal
# from PyQt5.QtGui import QImage, QPixmap, QFont, QColor
# from PyQt5.Qsci import QsciScintilla, QsciLexerPython
# import threading
# import elevate
# import sys
# import numpy as np

# # Set up logging
# logging.basicConfig(level=logging.DEBUG)
# logger = logging.getLogger(__name__)

# SERVER_URL = "wss://dashboard.intellirecruit.ai/websocket"
# CLIENT_ID = "client1"

# class VideoTransformTrack(VideoStreamTrack):
#     def __init__(self):
#         super().__init__()
#         self.cap = cv2.VideoCapture(0)
#         self.cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)
#         self.cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)
#         if not self.cap.isOpened():
#             logger.error("Could not start video capture")
#             raise RuntimeError("Could not start video capture")

#     async def recv(self):
#         pts, time_base = await self.next_timestamp()

#         ret, frame = self.cap.read()
#         if not ret:
#             logger.error("Failed to capture frame")
#             raise RuntimeError("Failed to capture frame")

#         frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
#         video_frame = VideoFrame.from_ndarray(frame, format="rgb24")
#         video_frame.pts = pts
#         video_frame.time_base = time_base

#         logger.debug("Captured frame")
#         return video_frame

# class WebRTCClient(QThread):
#     incoming_frame = pyqtSignal(np.ndarray)
#     outgoing_frame = pyqtSignal(np.ndarray)

#     def __init__(self):
#         super().__init__()
#         self.pc = None
#         self.video_track = None

#     async def run_offer(self, pc):
#         await pc.setLocalDescription(await pc.createOffer())
#         logger.debug(f"Local Description: {pc.localDescription.sdp}")
#         return {"sdp": pc.localDescription.sdp, "type": pc.localDescription.type}

#     async def consume_signaling(self, pc, websocket):
#         async for message in websocket:
#             logger.info(f"Received message: {message}")
#             msg = json.loads(message)
#             if msg["type"] == "answer":
#                 logger.debug(f"Answer SDP: {msg['answer']['sdp']}")
#                 await pc.setRemoteDescription(RTCSessionDescription(sdp=msg["answer"]["sdp"], type=msg["answer"]["type"]))
#             else:
#                 logger.warning(f"Unexpected message type: {msg['type']}")

#     async def main_webrtc(self):
#         recorder = MediaRecorder("received_video_client1.mp4")

#         self.pc = RTCPeerConnection()
#         self.video_track = VideoTransformTrack()
#         self.pc.addTrack(self.video_track)

#         @self.pc.on("track")
#         def on_track(track):
#             logger.info("Receiving video track")
#             if track.kind == "video":
#                 # Add incoming video track to recorder and display it
#                 recorder.addTrack(track)

#                 async def display_incoming_video():
#                     while True:
#                         frame = await track.recv()
#                         img = frame.to_ndarray(format="bgr24")
#                         self.incoming_frame.emit(img)

#                 asyncio.ensure_future(display_incoming_video())
#                 logger.debug("Added video track to recorder and displaying incoming video")

#         try:
#             logger.info(f"Attempting to connect to {SERVER_URL}")
#             async with websockets.connect(SERVER_URL) as websocket:
#                 logger.info("Connected to WebSocket server")

#                 await websocket.send(json.dumps({"type": "join", "client_id": CLIENT_ID}))
#                 logger.info(f"Sent join message for {CLIENT_ID}")

#                 offer = await self.run_offer(self.pc)
#                 await websocket.send(json.dumps({"type": "offer", "offer": offer}))
#                 logger.info("Sent offer")

#                 await self.consume_signaling(self.pc, websocket)

#                 await recorder.start()
#                 logger.info("Recorder started")

#                 while True:
#                     await asyncio.sleep(1)

#         except websockets.exceptions.ConnectionClosed as e:
#             logger.error(f"WebSocket connection closed unexpectedly: {e}")
#         except Exception as e:
#             logger.error(f"An error occurred: {e}")
#         finally:
#             logger.info("Cleaning up...")
#             await recorder.stop()
#             await self.pc.close()
#             self.video_track.cap.release()
#             cv2.destroyAllWindows()

#     def run(self):
#         asyncio.run(self.main_webrtc())

# class FullScreenWindow(QMainWindow):
#     def __init__(self):
#         super().__init__()

#         # Set window properties
#         self.setWindowTitle("Kiosk App")  # Change the window title
#         self.setWindowFlags(Qt.WindowCloseButtonHint | Qt.WindowStaysOnTopHint | Qt.FramelessWindowHint | Qt.WindowDoesNotAcceptFocus)
#         self.setFixedSize(1920, 1080)  # Set the size to fullscreen
        
#         # Create a central widget and layout
#         central_widget = QWidget(self)
#         self.setCentralWidget(central_widget)
#         main_layout = QHBoxLayout(central_widget)
        
#         # Create a layout for the left side (editor and output)
#         left_layout = QVBoxLayout()
        
#         # Create and add the code editor
#         self.code_editor = QsciScintilla(self)
#         self.setup_editor()
#         left_layout.addWidget(self.code_editor)
        
#         # Create and add the output display
#         self.output_display = QTextEdit(self)
#         self.output_display.setReadOnly(True)
#         left_layout.addWidget(self.output_display)
#         left_layout.setStretch(0, 2)  # Set the editor to take 2/3 of the space
#         left_layout.setStretch(1, 1)  # Set the output to take 1/3 of the space
        
#         main_layout.addLayout(left_layout)
        
#         # Create a layout for the right side (camera and buttons)
#         right_layout = QVBoxLayout()
        
#         # Create and add the run button
#         self.run_button = QPushButton("Run Code", self)
#         self.run_button.clicked.connect(self.run_code)
#         right_layout.addWidget(self.run_button)
        
#         # Add the camera display widgets
#         self.outgoing_camera_label = QLabel(self)
#         right_layout.addWidget(self.outgoing_camera_label)

#         self.incoming_camera_label = QLabel(self)
#         right_layout.addWidget(self.incoming_camera_label)

#         # Add a warning section
#         self.warning_label = QLabel(self)
#         self.warning_label.setStyleSheet("color: red; font-size: 16px;")
#         right_layout.addWidget(self.warning_label)
        
#         main_layout.addLayout(right_layout)

#         # Set up the camera
#         self.cap = cv2.VideoCapture(0)
#         self.timer = QTimer()
#         self.timer.timeout.connect(self.update_outgoing_frame)
#         self.timer.start(30)  # Update the frame every 30 ms

#         # Set up WebRTC client
#         self.webrtc_client = WebRTCClient()
#         self.webrtc_client.incoming_frame.connect(self.update_incoming_frame)
#         self.webrtc_client.start()

    # def setup_editor(self):
    #     # Set up the lexer for syntax highlighting
    #     lexer = QsciLexerPython()
    #     self.code_editor.setLexer(lexer)
        
    #     # Set editor properties
    #     self.code_editor.setUtf8(True)  # Ensure Unicode support
    #     self.code_editor.setMarginsFont(QFont("Consolas", 12))  # Set font for line numbers
    #     self.code_editor.setMarginLineNumbers(1, True)  # Show line numbers
    #     self.code_editor.setMarginsBackgroundColor(QColor("#2e2e2e"))  # Set background color for line numbers area
    #     self.code_editor.setMarginWidth(1, 50)  # Set width of line numbers area
    #     self.code_editor.setBraceMatching(QsciScintilla.SloppyBraceMatch)  # Enable brace matching
    #     self.code_editor.setCaretLineVisible(True)  # Show a line under the cursor
    #     self.code_editor.setCaretLineBackgroundColor(QColor("#393939"))  # Set color for the line under the cursor
        
    #     # Set color scheme for syntax highlighting
    #     self.code_editor.setPaper(QColor("#1e1e1e"))  # Set editor background color
    #     lexer.setDefaultColor(QColor("#f8f8f2"))  # Set default text color
    #     lexer.setColor(QColor("#f8f8f2"), QsciLexerPython.Default)  # Set default color for Python code
    #     lexer.setColor(QColor("#66d9ef"), QsciLexerPython.Keyword)  # Set color for Python keywords
    #     lexer.setColor(QColor("#75715e"), QsciLexerPython.Comment)  # Set color for comments
    #     lexer.setColor(QColor("#e6db74"), QsciLexerPython.DoubleQuotedString)  # Set color for double quoted strings
    #     lexer.setColor(QColor("#e6db74"), QsciLexerPython.SingleQuotedString)  # Set color for single quoted strings
    #     lexer.setColor(QColor("#ae81ff"), QsciLexerPython.Number)  # Set color for numbers
    #     lexer.setColor(QColor("#ae81ff"), QsciLexerPython.TripleSingleQuotedString)  # Set color for triple single quoted strings
    #     lexer.setColor(QColor("#ae81ff"), QsciLexerPython.TripleDoubleQuotedString)  # Set color for triple double quoted strings
    #     lexer.setColor(QColor("#f8f8f2"), QsciLexerPython.ClassName)  # Set color for class names
    #     lexer.setColor(QColor("#f8f8f2"), QsciLexerPython.FunctionMethodName)  # Set color for function/method names
    #     lexer.setColor(QColor("#f8f8f2"), QsciLexerPython.Operator)  # Set color for operators
    #     lexer.setColor(QColor("#f8f8f2"), QsciLexerPython.Identifier)  # Set color for identifiers
    #     lexer.setColor(QColor("#75715e"), QsciLexerPython.CommentBlock)  # Set color for comment blocks
    #     lexer.setColor(QColor("#e6db74"), QsciLexerPython.UnclosedString)  # Set color for unclosed strings

    # def run_code(self):
    #     # Get the code from the editor
    #     code = self.code_editor.text()
        
    #     # Run the code in a separate process
    #     process = QProcess(self)
    #     process.start("python", ["-c", code])
    #     process.waitForFinished()
        
    #     # Get the output and error messages
    #     output = process.readAllStandardOutput().data().decode()
    #     error = process.readAllStandardError().data().decode()
        
    #     # Display the output and error messages
    #     self.output_display.setText(output + error)

    #     # Display a warning message if there is an error
    #     if error:
    #         self.warning_label.setText("Warning: Errors encountered while running the code.")
    #     else:
    #         self.warning_label.setText("")  # Clear the warning message if no errors

#     @pyqtSlot()
#     def update_outgoing_frame(self):
#         # Read frame from the camera
#         ret, frame = self.cap.read()
        
#         # If frame is read successfully, display it
#         if ret:
#             frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
#             image = QImage(frame, frame.shape[1], frame.shape[0], QImage.Format_RGB888)
#             self.outgoing_camera_label.setPixmap(QPixmap.fromImage(image))
#             self.webrtc_client.outgoing_frame.emit(frame)

#     @pyqtSlot(np.ndarray)
#     def update_incoming_frame(self, frame):
#         # Display the incoming frame
#         image = QImage(frame, frame.shape[1], frame.shape[0], QImage.Format_RGB888)
#         self.incoming_camera_label.setPixmap(QPixmap.fromImage(image))

#     def closeEvent(self, event):
#         # Release the camera resources
#         self.cap.release()
#         event.ignore()  # Ignore any attempts to close the window

#     def keyPressEvent(self, event):
#         # Check if Ctrl+P is pressed
#         if event.key() == Qt.Key_P and event.modifiers() & Qt.ControlModifier:
#             QApplication.quit()  # Exit the application

# def start_app():
#     app = QApplication(sys.argv)
#     window = FullScreenWindow()
#     window.showFullScreen()  # Show the window in fullscreen mode
#     sys.exit(app.exec_())  

# def main_vad():
#     parser = argparse.ArgumentParser()
#     parser.add_argument("--energy_threshold", default=1000,
#                         help="Energy level for mic to detect.", type=int)
#     parser.add_argument("--record_timeout", default=2,
#                         help="How real time the recording is in seconds.", type=float)
#     if 'linux' in platform:
#         parser.add_argument("--default_microphone", default='pulse',
#                             help="Default microphone name for SpeechRecognition."
#                                  "Run this with 'list' to view available Microphones.", type=str)
#     args = parser.parse_args()
#     # Current raw audio bytes.
#     last_sample = bytes()
#     # Thread safe Queue for passing data from the threaded recording callback.
#     data_queue = Queue()
#     # We use SpeechRecognizer to record our audio because it has a nice feauture where it can detect when speech ends.
#     recorder = sr.Recognizer()
#     recorder.energy_threshold = args.energy_threshold
#     # Definitely do this, dynamic energy compensation lowers the energy threshold dramtically to a point where the SpeechRecognizer never stops recording.
#     recorder.dynamic_energy_threshold = False
    
#     # Important for linux users. 
#     # Prevents permanent application hang and crash by using the wrong Microphone
#     if 'linux' in platform:
#         mic_name = args.default_microphone
#         if not mic_name or mic_name == 'list':
#             print("Available microphone devices are: ")
#             for index, name in enumerate(sr.Microphone.list_microphone_names()):
#                 print(f"Microphone with name \"{name}\" found")
#             return
#         else:
#             for index, name in enumerate(sr.Microphone.list_microphone_names()):
#                 if mic_name in name:
#                     source = sr.Microphone(sample_rate=16000, device_index=index)
#                     break
#     else:
#         source = sr.Microphone(sample_rate=16000)

#     record_timeout = args.record_timeout

#     temp_file = NamedTemporaryFile().name
    
#     with source:
#         recorder.adjust_for_ambient_noise(source)
#         print("Adjusted for ambient noise.")

#     def record_callback(_, audio: sr.AudioData) -> None:
#         """
#         Threaded callback function to receive audio data when recordings finish.
#         audio: An AudioData containing the recorded bytes.
#         """
#         # Grab the raw bytes and push it into the thread safe queue.
#         data = audio.get_raw_data()
#         data_queue.put(data)
#         print("Audio data received and added to queue.")

#     model, utils = torch.hub.load(repo_or_dir='snakers4/silero-vad',
#                                   model='silero_vad',
#                                   force_reload=False)
#     (get_speech_timestamps, save_audio, read_audio, VADIterator, collect_chunks) = utils
    
#     # Cue the user that we're ready to go.
#     print("Model loaded.\n")

#     # Start the recording process
#     recorder.listen_in_background(source, record_callback, phrase_time_limit=record_timeout)
#     print("Recording started.")

#     while True:
#         try:
#             now = datetime.utcnow()
#             # Pull raw recorded audio from the queue.
#             if not data_queue.empty():
#                 print("Data queue is not empty, processing audio data.")
#                 # Concatenate our current audio data with the latest audio data.
#                 while not data_queue.empty():
#                     data = data_queue.get()
#                     last_sample += data

#                 # Use AudioData to convert the raw data to wav data.
#                 audio_data = sr.AudioData(last_sample, source.SAMPLE_RATE, source.SAMPLE_WIDTH)
#                 wav_file = audio_data.get_wav_data()
#                 wav_data = io.BytesIO(audio_data.get_wav_data())

#                 # Write wav data to the temporary file as bytes.
#                 with open(temp_file, 'w+b') as f:
#                     f.write(wav_data.read())

#                 # Read the transcription.
#                 wav = read_audio(temp_file, sampling_rate=16000)
#                 speech_timestamps = get_speech_timestamps(wav, model, sampling_rate=16000)

#                 if speech_timestamps:
#                     print('Speech Detected!')
#                 else:
#                     print('Silence Detected!')
#             else:
#                 print("Data queue is empty.")
#         except KeyboardInterrupt:
#             break

# if __name__ == "__main__":
#     elevate.elevate()
#     thread1 = threading.Thread(target=start_app, name='Thread 1')
#     thread2 = threading.Thread(target=main_vad, name='Thread 2')
#     thread3 = threading.Thread(target=lambda: asyncio.run(WebRTCClient().main_webrtc()), name='Thread 3')

#     # Start the threads
#     thread1.start()
#     thread2.start()
#     thread3.start()

#     # Wait for all threads to finish
#     thread1.join()
#     thread2.join()
#     thread3.join()


import argparse
import io
import speech_recognition as sr
import torch
import asyncio
import json
import cv2
from aiortc import RTCPeerConnection, RTCSessionDescription, VideoStreamTrack
from aiortc.contrib.media import MediaRecorder
import websockets
from av import VideoFrame
import logging
from datetime import datetime
from queue import Queue
from tempfile import NamedTemporaryFile
from sys import platform
from PyQt5.QtWidgets import QApplication, QMainWindow, QVBoxLayout, QWidget, QPushButton, QHBoxLayout, QLabel, QTextEdit
from PyQt5.QtCore import Qt, QTimer, pyqtSlot, QThread, pyqtSignal
from PyQt5.QtGui import QImage, QPixmap, QFont, QColor
from PyQt5.Qsci import QsciScintilla, QsciLexerPython
import threading
import elevate
import sys
import numpy as np

# Set up logging
logging.basicConfig(level=logging.DEBUG)
logger = logging.getLogger(__name__)

SERVER_URL = "wss://dashboard.intellirecruit.ai/websocket"
CLIENT_ID = "client1"

class CameraManager:
    def __init__(self):
        self.cap = cv2.VideoCapture(0)
        self.cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)
        self.cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)
        self.lock = threading.Lock()

    def read_frame(self):
        with self.lock:
            ret, frame = self.cap.read()
        if ret:
            return cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        return None

    def release(self):
        self.cap.release()

camera_manager = CameraManager()

class VideoTransformTrack(VideoStreamTrack):
    def __init__(self):
        super().__init__()

    async def recv(self):
        pts, time_base = await self.next_timestamp()

        frame = camera_manager.read_frame()
        if frame is None:
            logger.error("Failed to capture frame")
            raise RuntimeError("Failed to capture frame")

        video_frame = VideoFrame.from_ndarray(frame, format="rgb24")
        video_frame.pts = pts
        video_frame.time_base = time_base

        logger.debug("Captured frame")
        return video_frame

class WebRTCClient(QThread):
    incoming_frame = pyqtSignal(np.ndarray)
    outgoing_frame = pyqtSignal(np.ndarray)

    def __init__(self):
        super().__init__()
        self.pc = None
        self.video_track = None

    async def run_offer(self, pc):
        await pc.setLocalDescription(await pc.createOffer())
        logger.debug(f"Local Description: {pc.localDescription.sdp}")
        return {"sdp": pc.localDescription.sdp, "type": pc.localDescription.type}

    async def consume_signaling(self, pc, websocket):
        async for message in websocket:
            logger.info(f"Received message: {message}")
            msg = json.loads(message)
            if msg["type"] == "answer":
                logger.debug(f"Answer SDP: {msg['answer']['sdp']}")
                await pc.setRemoteDescription(RTCSessionDescription(sdp=msg["answer"]["sdp"], type=msg["answer"]["type"]))
            else:
                logger.warning(f"Unexpected message type: {msg['type']}")

    async def main_webrtc(self):
        recorder = MediaRecorder("received_video_client1.mp4")

        self.pc = RTCPeerConnection()
        self.video_track = VideoTransformTrack()
        self.pc.addTrack(self.video_track)

        @self.pc.on("track")
        def on_track(track):
            logger.info("Receiving video track")
            if track.kind == "video":
                recorder.addTrack(track)

                async def display_incoming_video():
                    while True:
                        frame = await track.recv()
                        img = frame.to_ndarray(format="bgr24")
                        self.incoming_frame.emit(img)

                asyncio.ensure_future(display_incoming_video())
                logger.debug("Added video track to recorder and displaying incoming video")

        try:
            logger.info(f"Attempting to connect to {SERVER_URL}")
            async with websockets.connect(SERVER_URL) as websocket:
                logger.info("Connected to WebSocket server")

                await websocket.send(json.dumps({"type": "join", "client_id": CLIENT_ID}))
                logger.info(f"Sent join message for {CLIENT_ID}")

                offer = await self.run_offer(self.pc)
                await websocket.send(json.dumps({"type": "offer", "offer": offer}))
                logger.info("Sent offer")

                await self.consume_signaling(self.pc, websocket)

                await recorder.start()
                logger.info("Recorder started")

                while True:
                    await asyncio.sleep(1)

        except websockets.exceptions.ConnectionClosed as e:
            logger.error(f"WebSocket connection closed unexpectedly: {e}")
        except Exception as e:
            logger.error(f"An error occurred: {e}")
        finally:
            logger.info("Cleaning up...")
            await recorder.stop()
            await self.pc.close()

    def run(self):
        asyncio.run(self.main_webrtc())

class FullScreenWindow(QMainWindow):
    def __init__(self):
        super().__init__()

        self.setWindowTitle("Kiosk App")
        self.setWindowFlags(Qt.WindowCloseButtonHint | Qt.WindowStaysOnTopHint | Qt.FramelessWindowHint | Qt.WindowDoesNotAcceptFocus)
        self.setFixedSize(1920, 1080)
        
        central_widget = QWidget(self)
        self.setCentralWidget(central_widget)
        main_layout = QHBoxLayout(central_widget)
        
        left_layout = QVBoxLayout()
        
        self.code_editor = QsciScintilla(self)
        self.setup_editor()
        left_layout.addWidget(self.code_editor)
        
        self.output_display = QTextEdit(self)
        self.output_display.setReadOnly(True)
        left_layout.addWidget(self.output_display)
        left_layout.setStretch(0, 2)
        left_layout.setStretch(1, 1)
        
        main_layout.addLayout(left_layout)
        
        right_layout = QVBoxLayout()
        
        self.run_button = QPushButton("Run Code", self)
        self.run_button.clicked.connect(self.run_code)
        right_layout.addWidget(self.run_button)
        
        self.outgoing_camera_label = QLabel(self)
        right_layout.addWidget(self.outgoing_camera_label)

        self.incoming_camera_label = QLabel(self)
        right_layout.addWidget(self.incoming_camera_label)

        self.warning_label = QLabel(self)
        self.warning_label.setStyleSheet("color: red; font-size: 16px;")
        right_layout.addWidget(self.warning_label)
        
        main_layout.addLayout(right_layout)

        self.timer = QTimer()
        self.timer.timeout.connect(self.update_outgoing_frame)
        self.timer.start(30)  # Update the frame every 30 ms

        self.webrtc_client = WebRTCClient()
        self.webrtc_client.incoming_frame.connect(self.update_incoming_frame)
        self.webrtc_client.start()

    def setup_editor(self):
        # Set up the lexer for syntax highlighting
        lexer = QsciLexerPython()
        self.code_editor.setLexer(lexer)
        
        # Set editor properties
        self.code_editor.setUtf8(True)  # Ensure Unicode support
        self.code_editor.setMarginsFont(QFont("Consolas", 12))  # Set font for line numbers
        self.code_editor.setMarginLineNumbers(1, True)  # Show line numbers
        self.code_editor.setMarginsBackgroundColor(QColor("#2e2e2e"))  # Set background color for line numbers area
        self.code_editor.setMarginWidth(1, 50)  # Set width of line numbers area
        self.code_editor.setBraceMatching(QsciScintilla.SloppyBraceMatch)  # Enable brace matching
        self.code_editor.setCaretLineVisible(True)  # Show a line under the cursor
        self.code_editor.setCaretLineBackgroundColor(QColor("#393939"))  # Set color for the line under the cursor
        
        # Set color scheme for syntax highlighting
        self.code_editor.setPaper(QColor("#1e1e1e"))  # Set editor background color
        lexer.setDefaultColor(QColor("#f8f8f2"))  # Set default text color
        lexer.setColor(QColor("#f8f8f2"), QsciLexerPython.Default)  # Set default color for Python code
        lexer.setColor(QColor("#66d9ef"), QsciLexerPython.Keyword)  # Set color for Python keywords
        lexer.setColor(QColor("#75715e"), QsciLexerPython.Comment)  # Set color for comments
        lexer.setColor(QColor("#e6db74"), QsciLexerPython.DoubleQuotedString)  # Set color for double quoted strings
        lexer.setColor(QColor("#e6db74"), QsciLexerPython.SingleQuotedString)  # Set color for single quoted strings
        lexer.setColor(QColor("#ae81ff"), QsciLexerPython.Number)  # Set color for numbers
        lexer.setColor(QColor("#ae81ff"), QsciLexerPython.TripleSingleQuotedString)  # Set color for triple single quoted strings
        lexer.setColor(QColor("#ae81ff"), QsciLexerPython.TripleDoubleQuotedString)  # Set color for triple double quoted strings
        lexer.setColor(QColor("#f8f8f2"), QsciLexerPython.ClassName)  # Set color for class names
        lexer.setColor(QColor("#f8f8f2"), QsciLexerPython.FunctionMethodName)  # Set color for function/method names
        lexer.setColor(QColor("#f8f8f2"), QsciLexerPython.Operator)  # Set color for operators
        lexer.setColor(QColor("#f8f8f2"), QsciLexerPython.Identifier)  # Set color for identifiers
        lexer.setColor(QColor("#75715e"), QsciLexerPython.CommentBlock)  # Set color for comment blocks
        lexer.setColor(QColor("#e6db74"), QsciLexerPython.UnclosedString)  # Set color for unclosed strings

    def run_code(self):
        # Get the code from the editor
        code = self.code_editor.text()
        
        # Run the code in a separate process
        process = QProcess(self)
        process.start("python", ["-c", code])
        process.waitForFinished()
        
        # Get the output and error messages
        output = process.readAllStandardOutput().data().decode()
        error = process.readAllStandardError().data().decode()
        
        # Display the output and error messages
        self.output_display.setText(output + error)

        # Display a warning message if there is an error
        if error:
            self.warning_label.setText("Warning: Errors encountered while running the code.")
        else:
            self.warning_label.setText("")  # Clear the warning message if no errors


    @pyqtSlot()
    def update_outgoing_frame(self):
        frame = camera_manager.read_frame()
        if frame is not None:
            image = QImage(frame, frame.shape[1], frame.shape[0], QImage.Format_RGB888)
            self.outgoing_camera_label.setPixmap(QPixmap.fromImage(image))
            self.webrtc_client.outgoing_frame.emit(frame)

    @pyqtSlot(np.ndarray)
    def update_incoming_frame(self, frame):
        image = QImage(frame, frame.shape[1], frame.shape[0], QImage.Format_RGB888)
        self.incoming_camera_label.setPixmap(QPixmap.fromImage(image))

    def closeEvent(self, event):
        event.ignore()

    def keyPressEvent(self, event):
        if event.key() == Qt.Key_P and event.modifiers() & Qt.ControlModifier:
            QApplication.quit()

def start_app():
    app = QApplication(sys.argv)
    window = FullScreenWindow()
    window.showFullScreen()
    sys.exit(app.exec_())  

def main_vad():
    parser = argparse.ArgumentParser()
    parser.add_argument("--energy_threshold", default=1000,
                        help="Energy level for mic to detect.", type=int)
    parser.add_argument("--record_timeout", default=2,
                        help="How real time the recording is in seconds.", type=float)
    if 'linux' in platform:
        parser.add_argument("--default_microphone", default='pulse',
                            help="Default microphone name for SpeechRecognition."
                                 "Run this with 'list' to view available Microphones.", type=str)
    args = parser.parse_args()
    # Current raw audio bytes.
    last_sample = bytes()
    # Thread safe Queue for passing data from the threaded recording callback.
    data_queue = Queue()
    # We use SpeechRecognizer to record our audio because it has a nice feauture where it can detect when speech ends.
    recorder = sr.Recognizer()
    recorder.energy_threshold = args.energy_threshold
    # Definitely do this, dynamic energy compensation lowers the energy threshold dramtically to a point where the SpeechRecognizer never stops recording.
    recorder.dynamic_energy_threshold = False
    
    # Important for linux users. 
    # Prevents permanent application hang and crash by using the wrong Microphone
    if 'linux' in platform:
        mic_name = args.default_microphone
        if not mic_name or mic_name == 'list':
            print("Available microphone devices are: ")
            for index, name in enumerate(sr.Microphone.list_microphone_names()):
                print(f"Microphone with name \"{name}\" found")
            return
        else:
            for index, name in enumerate(sr.Microphone.list_microphone_names()):
                if mic_name in name:
                    source = sr.Microphone(sample_rate=16000, device_index=index)
                    break
    else:
        source = sr.Microphone(sample_rate=16000)

    record_timeout = args.record_timeout

    temp_file = NamedTemporaryFile().name
    
    with source:
        recorder.adjust_for_ambient_noise(source)
        print("Adjusted for ambient noise.")

    def record_callback(_, audio: sr.AudioData) -> None:
        """
        Threaded callback function to receive audio data when recordings finish.
        audio: An AudioData containing the recorded bytes.
        """
        # Grab the raw bytes and push it into the thread safe queue.
        data = audio.get_raw_data()
        data_queue.put(data)
        print("Audio data received and added to queue.")

    model, utils = torch.hub.load(repo_or_dir='snakers4/silero-vad',
                                  model='silero_vad',
                                  force_reload=False)
    (get_speech_timestamps, save_audio, read_audio, VADIterator, collect_chunks) = utils
    
    # Cue the user that we're ready to go.
    print("Model loaded.\n")

    # Start the recording process
    recorder.listen_in_background(source, record_callback, phrase_time_limit=record_timeout)
    print("Recording started.")

    while True:
        try:
            now = datetime.utcnow()
            # Pull raw recorded audio from the queue.
            if not data_queue.empty():
                print("Data queue is not empty, processing audio data.")
                # Concatenate our current audio data with the latest audio data.
                while not data_queue.empty():
                    data = data_queue.get()
                    last_sample += data

                # Use AudioData to convert the raw data to wav data.
                audio_data = sr.AudioData(last_sample, source.SAMPLE_RATE, source.SAMPLE_WIDTH)
                wav_file = audio_data.get_wav_data()
                wav_data = io.BytesIO(audio_data.get_wav_data())

                # Write wav data to the temporary file as bytes.
                with open(temp_file, 'w+b') as f:
                    f.write(wav_data.read())

                # Read the transcription.
                wav = read_audio(temp_file, sampling_rate=16000)
                speech_timestamps = get_speech_timestamps(wav, model, sampling_rate=16000)

                if speech_timestamps:
                    print('Speech Detected!')
                else:
                    print('Silence Detected!')
            else:
                print("Data queue is empty.")
        except KeyboardInterrupt:
            break


if __name__ == "__main__":
    elevate.elevate()
    thread1 = threading.Thread(target=start_app, name='Thread 1')
    thread2 = threading.Thread(target=main_vad, name='Thread 2')
    thread3 = threading.Thread(target=lambda: asyncio.run(WebRTCClient().main_webrtc()), name='Thread 3')

    thread1.start()
    thread2.start()
    thread3.start()

    thread1.join()
    thread2.join()
    thread3.join()

    camera_manager.release()